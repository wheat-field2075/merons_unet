{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mmfs1/data/linok/.conda/envs/merons/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from tensorboardX import SummaryWriter\n",
    "import albumentations as A\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('./modules')\n",
    "\n",
    "from UNet import UNet\n",
    "from Dataset import Dataset\n",
    "from ImageLoader import ImageLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = yaml.load(open('./settings.yaml', 'r'), yaml.Loader)\n",
    "\n",
    "images_path = data['images_path']\n",
    "masks_path = data['masks_path']\n",
    "image_patches_path = data['image_patches_path']\n",
    "mask_patches_path = data['mask_patches_path']\n",
    "\n",
    "patch_size = data['patch_size']\n",
    "batch_size = data['batch_size']\n",
    "sigma = data['sigma']\n",
    "num_neg_samples = data['num_neg_samples']\n",
    "\n",
    "folder = '2023.06.09 focal_loss_v2'\n",
    "model_name = 'unet'\n",
    "\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.RandomRotate90(p=1),\n",
    "    A.Transpose(p=0.5),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "])\n",
    "\n",
    "train_set = []\n",
    "val_set = []\n",
    "\n",
    "for i in [10, 20, 30, 40]:\n",
    "    train_set.append(\"Bubbles_movie_01_x1987x2020x81_3cv2_NLM_template20_search62_inverted{}.png\".format(i))\n",
    "for i in [50]:\n",
    "    val_set.append(\"Bubbles_movie_01_x1987x2020x81_3cv2_NLM_template20_search62_inverted{}.png\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: Bubbles_movie_01_x1987x2020x81_3cv2_NLM_template20_search62_inverted10.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 368/368 [00:15<00:00, 24.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: Bubbles_movie_01_x1987x2020x81_3cv2_NLM_template20_search62_inverted20.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 444/444 [00:19<00:00, 23.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: Bubbles_movie_01_x1987x2020x81_3cv2_NLM_template20_search62_inverted30.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 392/392 [00:15<00:00, 24.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: Bubbles_movie_01_x1987x2020x81_3cv2_NLM_template20_search62_inverted40.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:17<00:00, 22.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: Bubbles_movie_01_x1987x2020x81_3cv2_NLM_template20_search62_inverted50.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 536/536 [00:23<00:00, 22.80it/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds = [[], []]\n",
    "val_ds = [[], []]\n",
    "\n",
    "for image_set, ds in [[train_set, train_ds], [val_set, val_ds]]:\n",
    "    for image in image_set:\n",
    "        print(\"Image:\", image)\n",
    "        patch_names = [file for file in os.listdir(os.path.join(image_patches_path, image)) if file[-4:] == '.npy']\n",
    "        for patch in tqdm(range(len(patch_names))):\n",
    "            image_patch = np.load(os.path.join(image_patches_path, image, patch_names[patch]))\n",
    "            mask_patch = np.load(os.path.join(mask_patches_path, image, patch_names[patch]))\n",
    "            \n",
    "            ds[0].append(np.expand_dims(image_patch, 0))\n",
    "            ds[1].append(np.expand_dims(mask_patch, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input image: a batched numpy array or torch Tensor with dimensions (batch_size, 1, H, W)\n",
    "# input mask: a batched numpy array or torch Tensor with dimensions (batch_size, 1, H, W)\n",
    "# input transform: a transformation that is applied to each image and corresponding masks\n",
    "# input scale_factor: the number of classes/intervals per mask\n",
    "# output image: a batch numpy array with transformations applied and with dimensions (batch_size, 1, H, W)\n",
    "# output mask_temp: a batch numpy array with transformations applied and with dimensions (batch_size, intervals, H, W)\n",
    "\n",
    "\n",
    "def transform_data(image, mask, transform, scale_factor):\n",
    "    if type(image) != np.array:\n",
    "        image = np.array(image)\n",
    "    if type(mask) != np.array:\n",
    "        mask = np.array(mask)\n",
    "    \n",
    "    for batch in range(image.shape[0]):\n",
    "        transformed = transform(image=np.moveaxis(image[batch], 0, -1), mask=np.moveaxis(mask[batch], 0, -1))\n",
    "        image[batch] = np.moveaxis(transformed['image'], -1, 0)\n",
    "        mask[batch] = np.moveaxis(transformed['mask'], -1, 0)\n",
    "    \n",
    "    mask_temp = np.zeros([mask.shape[0], intervals, mask.shape[2], mask.shape[3]])\n",
    "    \n",
    "    for interval in range(scale_factor):\n",
    "        lower_bound = (1 / (scale_factor - 1)) * interval\n",
    "        upper_bound = (1 / (scale_factor - 1)) * (interval + 1)\n",
    "        mask_temp[:, interval] = ((mask >= lower_bound) * (mask < upper_bound)).squeeze()\n",
    "        \n",
    "    return image, mask_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset(train_ds[0], train_ds[1])\n",
    "val_ds = Dataset(val_ds[0], val_ds[1])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, shuffle=True, batch_size=batch_size)\n",
    "val_loader = torch.utils.data.DataLoader(val_ds, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "epochs = 10000\n",
    "lr = 5e-1\n",
    "intervals = 9\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "unet = UNet(n_channels=1, n_classes=intervals).to(device)\n",
    "unet = unet.to(device)\n",
    "lossFunc = torchvision.ops.sigmoid_focal_loss\n",
    "opt = torch.optim.SGD(unet.parameters(), lr=lr)\n",
    "\n",
    "writer = SummaryWriter('./{}/runs/{}, lr={}'.format(folder, model_name, lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                       | 6/25000 [02:17<158:53:11, 22.89s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m x, y \u001b[38;5;241m=\u001b[39m transform_data(x, y, transform, intervals)\n\u001b[1;32m      8\u001b[0m x, y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(x), torch\u001b[38;5;241m.\u001b[39mTensor(y)\n\u001b[0;32m---> 10\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m     13\u001b[0m pred \u001b[38;5;241m=\u001b[39m unet(x)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(epochs)):\n",
    "    unet.train()\n",
    "    total_train_loss = 0\n",
    "    total_val_loss = 0\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        x, y = transform_data(x, y, transform, intervals)\n",
    "        x, y = torch.Tensor(x), torch.Tensor(y)\n",
    "\n",
    "        x = x.to(device, dtype=torch.float)\n",
    "        y = y.to(device, dtype=torch.float)\n",
    "        \n",
    "        pred = unet(x)\n",
    "        loss = lossFunc(y, pred).mean()\n",
    "        total_train_loss += loss\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        unet.eval()\n",
    "\n",
    "        for x, y in val_loader:\n",
    "            x, y = transform_data(x, y, transform, intervals)\n",
    "            x, y = torch.Tensor(x), torch.Tensor(y)\n",
    "\n",
    "            x = x.to(device, dtype=torch.float)\n",
    "            y = y.to(device, dtype=torch.float)\n",
    "\n",
    "            pred = unet(x)\n",
    "            loss = lossFunc(y, pred).mean()\n",
    "            total_val_loss += loss\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    \n",
    "    writer.add_scalar('train_loss', avg_train_loss, epoch)\n",
    "    writer.add_scalar('val_loss', avg_val_loss, epoch)\n",
    "    \n",
    "    if (epoch + 1) % 400 == 0:\n",
    "        model_param_path = './{}/model_saves/{}, lr={}, epoch={}.pth'.format(folder, model_name, lr, epoch + 1)\n",
    "        torch.save(unet.state_dict(), model_param_path)\n",
    "\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "merons",
   "language": "python",
   "name": "merons"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
